{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction of participant only audios from manual diarizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This scripts takes care of diarization of the audio files, via the pyannote-audio library.\n",
    "First, we extract the segmented speakers in the form of a text file. Then, we determine the \n",
    "speaker with the highest speaking time in seconds. We then extract the audio of the speaker \n",
    "and save it in a separate file.\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "from pyannote.audio import Pipeline\n",
    "import torch\n",
    "import torchaudio\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "\n",
    "from diarization.passwords import AUTH_TOKEN\n",
    "pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization-3.1\",\n",
    "                                    use_auth_token=AUTH_TOKEN)\n",
    "# check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    pipeline.to(torch.device(\"cuda\"))\n",
    "else:\n",
    "    pipeline.to(torch.device(\"cpu\"))\n",
    "\n",
    "def diarize(audio_file):\n",
    "    \"\"\"\n",
    "    This function takes an audio file as input and returns the diarization object,\n",
    "    diarization dataframe and a dictionary containing the number of speakers and \n",
    "    their speaking time.\"\"\"\n",
    "    waveform, sr = torchaudio.load(audio_file) # Audio must be a torch tensor\n",
    "    diarization = pipeline({'waveform':waveform,'sample_rate':sr})\n",
    "    diarization_df = pd.DataFrame()\n",
    "\n",
    "    # Count number of speakers\n",
    "    speakers = []\n",
    "    for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "        if speaker not in speakers:\n",
    "            speakers.append(speaker)\n",
    "\n",
    "        dur = turn.end-turn.start\n",
    "        new_row = {'start': int(turn.start * sr), \n",
    "                   'end': int(turn.end * sr), \n",
    "                   'start [s]': round(turn.start,3), \n",
    "                   'end [s]': round(turn.end,3), \n",
    "                   'dur [s]' : round(dur,3), \n",
    "                   'speaker': speaker}\n",
    "        diarization_df = pd.concat([diarization_df, pd.DataFrame(new_row, index=[0])], \n",
    "                                   ignore_index=True)\n",
    "\n",
    "    num_speakers = len(speakers)\n",
    "    speaker_durs = diarization_df.groupby(['speaker'], as_index=False).sum()\n",
    "\n",
    "    output_dict = dict()\n",
    "    output_dict['filename'] = audio_file\n",
    "    output_dict['num_speakers'] = num_speakers\n",
    "\n",
    "    for _, row in speaker_durs.iterrows():\n",
    "        output_dict[str(row['speaker'])] = row['dur [s]']\n",
    "\n",
    "    return diarization, diarization_df, output_dict\n",
    "\n",
    "def get_longest_speaker(diarization_df):\n",
    "    \"\"\"\n",
    "    This function takes the diarization dataframe as input and returns the speaker\n",
    "    with the highest speaking time in seconds.\"\"\"\n",
    "    speaker_durs = diarization_df.groupby(['speaker'], as_index=False).sum()\n",
    "    speaker_durs = speaker_durs.sort_values(by='dur [s]', ascending=False)\n",
    "    longest_speaker = speaker_durs.iloc[0]['speaker']\n",
    "    return longest_speaker\n",
    "\n",
    "def extract_audio(audio_file, diarization_df, longest_speaker):\n",
    "    \"\"\"\n",
    "    This function takes the audio file, diarization dataframe and the speaker with the\n",
    "    highest speaking time as input and deletes any intervention from other speakers.\n",
    "    Resulting audio should be of equal lenght to the original audio.\"\"\"\n",
    "    waveform, sr = sf.read(audio_file)\n",
    "    new_waveform = np.zeros_like(waveform)\n",
    "    for _, row in diarization_df.iterrows():\n",
    "        if row['speaker'] != longest_speaker:\n",
    "            new_waveform[row['start']:row['end']] = 0   # Delete audio from other speakers\n",
    "        else:\n",
    "            new_waveform[row['start']:row['end']] = waveform[row['start']:row['end']]\n",
    "    output_file = os.path.join(base_dir, 'diarization', f'{audio_file.split(\"/\")[-1][:-4]}_participant.wav')\n",
    "    sf.write(output_file, new_waveform, sr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_manual_diarization(base_dir, audio_path, manual_diarization_path):\n",
    "    \"\"\"\n",
    "    This function takes the audio file and the manual diarization file as input and deletes any intervention from other speakers.\n",
    "    Resulting audio should be of equal lenght to the original audio.\"\"\"\n",
    "    waveform, sr = sf.read(audio_path)\n",
    "    new_waveform = np.zeros_like(waveform)\n",
    "    with open(manual_diarization_path, 'r', encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            start, end, speaker = line.split()\n",
    "            speaker = speaker.lower()\n",
    "            if speaker == \"p\":\n",
    "                #keep audio from participant\n",
    "                start = int(float(start) * sr)\n",
    "                end = int(float(end) * sr)\n",
    "                new_waveform[start:end] = waveform[start:end]\n",
    "            else:\n",
    "                start = int(float(start) * sr)\n",
    "                end = int(float(end) * sr)\n",
    "                new_waveform[start:end] = 0\n",
    "    output_file = os.path.join(base_dir, f'{audio_path.split(\"/\")[-1][:-4]}_manual_participant.wav')\n",
    "    sf.write(output_file, new_waveform, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 37/276 [00:00<00:05, 41.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with Slachevsky_EP_15_Intereses.txt: Error opening '/home/aleph/diariziation_error_rate/combined_database/reference2/FA/Slachevsky_EP_15_Intereses.wav': System error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 230/276 [00:04<00:00, 47.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with FONDECYT_SL0349_Lectura.txt: Error opening '/home/aleph/diariziation_error_rate/combined_database/reference/SG/FONDECYT_SL0349_Lectura.wav': System error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 276/276 [00:06<00:00, 45.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# read der_report.csv\n",
    "\n",
    "der_report = pd.read_csv(\"DER_report.csv\")\n",
    "output_dir = \"/home/aleph/diariziation_error_rate/filtered_missed_detection\"\n",
    "\n",
    "# iterate over every filename in the \"filename\" column\n",
    "for filename in tqdm.tqdm(der_report['filename']):\n",
    "    # print(filename)\n",
    "    FOLDER = \"/home/aleph/diariziation_error_rate/combined_database/reference2\"\n",
    "    # search for the file in the folder\n",
    "    for dirpath, dirnames, filenames in os.walk(FOLDER):\n",
    "            if filename in filenames:\n",
    "                manual_diarization_path = os.path.join(dirpath, filename)\n",
    "                audio_path = os.path.join(dirpath, filename[:-4] + \".wav\")\n",
    "                break\n",
    "            else:\n",
    "                manual_diarization_path = None\n",
    "    # check if the file was found\n",
    "\n",
    "    if not manual_diarization_path:\n",
    "        #check in different folder\n",
    "        FOLDER = \"/home/aleph/diariziation_error_rate/combined_database/reference\"\n",
    "        for dirpath, dirnames, filenames in os.walk(FOLDER):\n",
    "            if filename in filenames:\n",
    "                manual_diarization_path = os.path.join(dirpath, filename)\n",
    "                audio_path = os.path.join(dirpath, filename[:-4] + \".wav\")\n",
    "                break\n",
    "            else:\n",
    "                manual_diarization_path = None\n",
    "    # check if the file was found\n",
    "    if not manual_diarization_path:\n",
    "        print(f\"{filename} not found\")\n",
    "        continue\n",
    "\n",
    "    # call to extract_audio_manual_diarization\n",
    "    try:\n",
    "        extract_audio_manual_diarization(output_dir, audio_path, manual_diarization_path) \n",
    "    # except LibsndfileError\n",
    "    except Exception as e:\n",
    "        print(f\"Error with {filename}: {e}\")\n",
    "        continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2936.73s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai-whisper\n",
      "  Downloading openai-whisper-20231117.tar.gz (798 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m798.6/798.6 KB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numba in ./.env/lib/python3.10/site-packages (from openai-whisper) (0.59.1)\n",
      "Collecting more-itertools\n",
      "  Downloading more_itertools-10.3.0-py3-none-any.whl (59 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.2/59.2 KB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in ./.env/lib/python3.10/site-packages (from openai-whisper) (4.66.2)\n",
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in ./.env/lib/python3.10/site-packages (from openai-whisper) (1.26.4)\n",
      "Requirement already satisfied: torch in ./.env/lib/python3.10/site-packages (from openai-whisper) (2.2.2)\n",
      "Requirement already satisfied: triton<3,>=2.0.0 in ./.env/lib/python3.10/site-packages (from openai-whisper) (2.2.0)\n",
      "Requirement already satisfied: filelock in ./.env/lib/python3.10/site-packages (from triton<3,>=2.0.0->openai-whisper) (3.13.4)\n",
      "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in ./.env/lib/python3.10/site-packages (from numba->openai-whisper) (0.42.0)\n",
      "Requirement already satisfied: requests>=2.26.0 in ./.env/lib/python3.10/site-packages (from tiktoken->openai-whisper) (2.31.0)\n",
      "Collecting regex>=2022.1.18\n",
      "  Downloading regex-2024.5.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (775 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m775.1/775.1 KB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.env/lib/python3.10/site-packages (from torch->openai-whisper) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./.env/lib/python3.10/site-packages (from torch->openai-whisper) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.env/lib/python3.10/site-packages (from torch->openai-whisper) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in ./.env/lib/python3.10/site-packages (from torch->openai-whisper) (2.19.3)\n",
      "Requirement already satisfied: sympy in ./.env/lib/python3.10/site-packages (from torch->openai-whisper) (1.12)\n",
      "Requirement already satisfied: jinja2 in ./.env/lib/python3.10/site-packages (from torch->openai-whisper) (3.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.env/lib/python3.10/site-packages (from torch->openai-whisper) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.env/lib/python3.10/site-packages (from torch->openai-whisper) (12.1.3.1)\n",
      "Requirement already satisfied: networkx in ./.env/lib/python3.10/site-packages (from torch->openai-whisper) (3.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.env/lib/python3.10/site-packages (from torch->openai-whisper) (12.1.105)\n",
      "Requirement already satisfied: fsspec in ./.env/lib/python3.10/site-packages (from torch->openai-whisper) (2024.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.env/lib/python3.10/site-packages (from torch->openai-whisper) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.env/lib/python3.10/site-packages (from torch->openai-whisper) (12.1.0.106)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.env/lib/python3.10/site-packages (from torch->openai-whisper) (4.11.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.env/lib/python3.10/site-packages (from torch->openai-whisper) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.env/lib/python3.10/site-packages (from torch->openai-whisper) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.env/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->openai-whisper) (12.4.127)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.env/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2024.2.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.env/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.env/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.env/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.env/lib/python3.10/site-packages (from jinja2->torch->openai-whisper) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./.env/lib/python3.10/site-packages (from sympy->torch->openai-whisper) (1.3.0)\n",
      "Building wheels for collected packages: openai-whisper\n",
      "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for openai-whisper: filename=openai_whisper-20231117-py3-none-any.whl size=801358 sha256=96baeff2371fb999768b9bf1095cc359d08d86bc44de5c134ebf82dd507bb05b\n",
      "  Stored in directory: /home/aleph/.cache/pip/wheels/d0/85/e1/9361b4cbea7dd4b7f6702fa4c3afc94877952eeb2b62f45f56\n",
      "Successfully built openai-whisper\n",
      "Installing collected packages: regex, more-itertools, tiktoken, openai-whisper\n",
      "Successfully installed more-itertools-10.3.0 openai-whisper-20231117 regex-2024.5.15 tiktoken-0.7.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U openai-whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 5.80 GiB of which 832.00 KiB is free. Including non-PyTorch memory, this process has 5.79 GiB memory in use. Of the allocated memory 5.21 GiB is allocated by PyTorch, and 433.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwhisper\u001b[39;00m\n\u001b[1;32m      6\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/aleph/diariziation_error_rate/filtered_missed_detection\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 8\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mwhisper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlarge\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(output_dir):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filename\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/diariziation_error_rate/.env/lib/python3.10/site-packages/whisper/__init__.py:146\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(name, device, download_root, in_memory)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found; available models = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavailable_models()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    141\u001b[0m     )\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m (\n\u001b[1;32m    144\u001b[0m     io\u001b[38;5;241m.\u001b[39mBytesIO(checkpoint_file) \u001b[38;5;28;01mif\u001b[39;00m in_memory \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(checkpoint_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    145\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[0;32m--> 146\u001b[0m     checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m checkpoint_file\n\u001b[1;32m    149\u001b[0m dims \u001b[38;5;241m=\u001b[39m ModelDimensions(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheckpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdims\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/diariziation_error_rate/.env/lib/python3.10/site-packages/torch/serialization.py:1026\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1024\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1025\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1026\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m                     \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[1;32m   1032\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmmap can only be used with files saved with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1033\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`torch.save(_use_new_zipfile_serialization=True), \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1034\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease torch.save your checkpoint with this option in order to use mmap.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/diariziation_error_rate/.env/lib/python3.10/site-packages/torch/serialization.py:1438\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1436\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1437\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1438\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1440\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1441\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_log_api_usage_metadata(\n\u001b[1;32m   1442\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.load.metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserialization_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: zip_file\u001b[38;5;241m.\u001b[39mserialization_id()}\n\u001b[1;32m   1443\u001b[0m )\n",
      "File \u001b[0;32m~/diariziation_error_rate/.env/lib/python3.10/site-packages/torch/serialization.py:1408\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1407\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1408\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1410\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[0;32m~/diariziation_error_rate/.env/lib/python3.10/site-packages/torch/serialization.py:1382\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1377\u001b[0m         storage\u001b[38;5;241m.\u001b[39mbyteswap(dtype)\n\u001b[1;32m   1379\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   1380\u001b[0m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   1381\u001b[0m typed_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mTypedStorage(\n\u001b[0;32m-> 1382\u001b[0m     wrap_storage\u001b[38;5;241m=\u001b[39m\u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1383\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   1384\u001b[0m     _internal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1386\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typed_storage\u001b[38;5;241m.\u001b[39m_data_ptr() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1387\u001b[0m     loaded_storages[key] \u001b[38;5;241m=\u001b[39m typed_storage\n",
      "File \u001b[0;32m~/diariziation_error_rate/.env/lib/python3.10/site-packages/torch/serialization.py:1308\u001b[0m, in \u001b[0;36m_get_restore_location.<locals>.restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrestore_location\u001b[39m(storage, location):\n\u001b[0;32m-> 1308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdefault_restore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/diariziation_error_rate/.env/lib/python3.10/site-packages/torch/serialization.py:391\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 391\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    393\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/diariziation_error_rate/.env/lib/python3.10/site-packages/torch/serialization.py:271\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mUntypedStorage(obj\u001b[38;5;241m.\u001b[39mnbytes(), device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(location))\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 271\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/diariziation_error_rate/.env/lib/python3.10/site-packages/torch/_utils.py:115\u001b[0m, in \u001b[0;36m_cuda\u001b[0;34m(self, device, non_blocking, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m new_type(indices, values, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 115\u001b[0m     untyped_storage \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUntypedStorage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m     untyped_storage\u001b[38;5;241m.\u001b[39mcopy_(\u001b[38;5;28mself\u001b[39m, non_blocking)\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m untyped_storage\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 5.80 GiB of which 832.00 KiB is free. Including non-PyTorch memory, this process has 5.79 GiB memory in use. Of the allocated memory 5.21 GiB is allocated by PyTorch, and 433.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# use whisper from open ai to extract the transcription of the audio file in the output_dir\n",
    "\n",
    "#EXAMPLE\n",
    "import os\n",
    "import whisper\n",
    "output_dir = \"/home/aleph/diariziation_error_rate/filtered_missed_detection\"\n",
    "\n",
    "model = whisper.load_model(\"large\")\n",
    "\n",
    "for filename in os.listdir(output_dir):\n",
    "    if filename.endswith(\".wav\"):\n",
    "        result = model.transcribe(os.path.join(output_dir, filename))\n",
    "        # save the transcription in a file\n",
    "        with open(os.path.join(output_dir, filename[:-4] + \".txt\"), \"w\") as f:\n",
    "            f.write(result[\"text\"])\n",
    "            \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
