{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction of participant only audios from manual diarizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This scripts takes care of diarization of the audio files, via the pyannote-audio library.\n",
    "First, we extract the segmented speakers in the form of a text file. Then, we determine the \n",
    "speaker with the highest speaking time in seconds. We then extract the audio of the speaker \n",
    "and save it in a separate file.\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "from pyannote.audio import Pipeline\n",
    "import torch\n",
    "import torchaudio\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "\n",
    "from diarization.passwords import AUTH_TOKEN\n",
    "pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization-3.1\",\n",
    "                                    use_auth_token=AUTH_TOKEN)\n",
    "# check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    pipeline.to(torch.device(\"cuda\"))\n",
    "else:\n",
    "    pipeline.to(torch.device(\"cpu\"))\n",
    "\n",
    "def diarize(audio_file):\n",
    "    \"\"\"\n",
    "    This function takes an audio file as input and returns the diarization object,\n",
    "    diarization dataframe and a dictionary containing the number of speakers and \n",
    "    their speaking time.\"\"\"\n",
    "    waveform, sr = torchaudio.load(audio_file) # Audio must be a torch tensor\n",
    "    diarization = pipeline({'waveform':waveform,'sample_rate':sr})\n",
    "    diarization_df = pd.DataFrame()\n",
    "\n",
    "    # Count number of speakers\n",
    "    speakers = []\n",
    "    for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "        if speaker not in speakers:\n",
    "            speakers.append(speaker)\n",
    "\n",
    "        dur = turn.end-turn.start\n",
    "        new_row = {'start': int(turn.start * sr), \n",
    "                   'end': int(turn.end * sr), \n",
    "                   'start [s]': round(turn.start,3), \n",
    "                   'end [s]': round(turn.end,3), \n",
    "                   'dur [s]' : round(dur,3), \n",
    "                   'speaker': speaker}\n",
    "        diarization_df = pd.concat([diarization_df, pd.DataFrame(new_row, index=[0])], \n",
    "                                   ignore_index=True)\n",
    "\n",
    "    num_speakers = len(speakers)\n",
    "    speaker_durs = diarization_df.groupby(['speaker'], as_index=False).sum()\n",
    "\n",
    "    output_dict = dict()\n",
    "    output_dict['filename'] = audio_file\n",
    "    output_dict['num_speakers'] = num_speakers\n",
    "\n",
    "    for _, row in speaker_durs.iterrows():\n",
    "        output_dict[str(row['speaker'])] = row['dur [s]']\n",
    "\n",
    "    return diarization, diarization_df, output_dict\n",
    "\n",
    "def get_longest_speaker(diarization_df):\n",
    "    \"\"\"\n",
    "    This function takes the diarization dataframe as input and returns the speaker\n",
    "    with the highest speaking time in seconds.\"\"\"\n",
    "    speaker_durs = diarization_df.groupby(['speaker'], as_index=False).sum()\n",
    "    speaker_durs = speaker_durs.sort_values(by='dur [s]', ascending=False)\n",
    "    longest_speaker = speaker_durs.iloc[0]['speaker']\n",
    "    return longest_speaker\n",
    "\n",
    "def extract_audio(audio_file, diarization_df, longest_speaker):\n",
    "    \"\"\"\n",
    "    This function takes the audio file, diarization dataframe and the speaker with the\n",
    "    highest speaking time as input and deletes any intervention from other speakers.\n",
    "    Resulting audio should be of equal lenght to the original audio.\"\"\"\n",
    "    waveform, sr = sf.read(audio_file)\n",
    "    new_waveform = np.zeros_like(waveform)\n",
    "    for _, row in diarization_df.iterrows():\n",
    "        if row['speaker'] != longest_speaker:\n",
    "            new_waveform[row['start']:row['end']] = 0   # Delete audio from other speakers\n",
    "        else:\n",
    "            new_waveform[row['start']:row['end']] = waveform[row['start']:row['end']]\n",
    "    output_file = os.path.join(base_dir, 'diarization', f'{audio_file.split(\"/\")[-1][:-4]}_participant.wav')\n",
    "    sf.write(output_file, new_waveform, sr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_manual_diarization(base_dir, audio_path, manual_diarization_path):\n",
    "    \"\"\"\n",
    "    This function takes the audio file and the manual diarization file as input and deletes any intervention from other speakers.\n",
    "    Resulting audio should be of equal lenght to the original audio.\"\"\"\n",
    "    waveform, sr = sf.read(audio_path)\n",
    "    new_waveform = np.zeros_like(waveform)\n",
    "    with open(manual_diarization_path, 'r', encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            start, end, speaker = line.split()\n",
    "            speaker = speaker.lower()\n",
    "            if speaker == \"p\":\n",
    "                #keep audio from participant\n",
    "                start = int(float(start) * sr)\n",
    "                end = int(float(end) * sr)\n",
    "                new_waveform[start:end] = waveform[start:end]\n",
    "            else:\n",
    "                start = int(float(start) * sr)\n",
    "                end = int(float(end) * sr)\n",
    "                new_waveform[start:end] = 0\n",
    "    output_file = os.path.join(base_dir, f'{audio_path.split(\"/\")[-1][:-4]}_manual_participant.wav')\n",
    "    sf.write(output_file, new_waveform, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 37/276 [00:00<00:05, 41.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with Slachevsky_EP_15_Intereses.txt: Error opening '/home/aleph/diariziation_error_rate/combined_database/reference2/FA/Slachevsky_EP_15_Intereses.wav': System error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 230/276 [00:04<00:00, 47.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with FONDECYT_SL0349_Lectura.txt: Error opening '/home/aleph/diariziation_error_rate/combined_database/reference/SG/FONDECYT_SL0349_Lectura.wav': System error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 276/276 [00:06<00:00, 45.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# read der_report.csv\n",
    "\n",
    "der_report = pd.read_csv(\"DER_report.csv\")\n",
    "output_dir = \"/home/aleph/diariziation_error_rate/filtered_missed_detection\"\n",
    "\n",
    "# iterate over every filename in the \"filename\" column\n",
    "for filename in tqdm.tqdm(der_report['filename']):\n",
    "    # print(filename)\n",
    "    FOLDER = \"/home/aleph/diariziation_error_rate/combined_database/reference2\"\n",
    "    # search for the file in the folder\n",
    "    for dirpath, dirnames, filenames in os.walk(FOLDER):\n",
    "            if filename in filenames:\n",
    "                manual_diarization_path = os.path.join(dirpath, filename)\n",
    "                audio_path = os.path.join(dirpath, filename[:-4] + \".wav\")\n",
    "                break\n",
    "            else:\n",
    "                manual_diarization_path = None\n",
    "    # check if the file was found\n",
    "\n",
    "    if not manual_diarization_path:\n",
    "        #check in different folder\n",
    "        FOLDER = \"/home/aleph/diariziation_error_rate/combined_database/reference\"\n",
    "        for dirpath, dirnames, filenames in os.walk(FOLDER):\n",
    "            if filename in filenames:\n",
    "                manual_diarization_path = os.path.join(dirpath, filename)\n",
    "                audio_path = os.path.join(dirpath, filename[:-4] + \".wav\")\n",
    "                break\n",
    "            else:\n",
    "                manual_diarization_path = None\n",
    "    # check if the file was found\n",
    "    if not manual_diarization_path:\n",
    "        print(f\"{filename} not found\")\n",
    "        continue\n",
    "\n",
    "    # call to extract_audio_manual_diarization\n",
    "    try:\n",
    "        extract_audio_manual_diarization(output_dir, audio_path, manual_diarization_path) \n",
    "    # except LibsndfileError\n",
    "    except Exception as e:\n",
    "        print(f\"Error with {filename}: {e}\")\n",
    "        continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vad the audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyannote.audio import Model\n",
    "from diarization.passwords import AUTH_TOKEN\n",
    "from pyannote.audio.pipelines import VoiceActivityDetection\n",
    "from pyannote.audio import Pipeline\n",
    "\n",
    "model = Model.from_pretrained(\n",
    "  \"pyannote/segmentation-3.0\", \n",
    "  use_auth_token=AUTH_TOKEN)\n",
    "\n",
    "\n",
    "\n",
    "def vad_pyannote(audio_path):\n",
    "  \n",
    "  HYPER_PARAMETERS = {\n",
    "    # remove speech regions shorter than that many seconds.\n",
    "    \"min_duration_on\": 0.0,\n",
    "    # fill non-speech regions shorter than that many seconds.\n",
    "    \"min_duration_off\": 0.0\n",
    "  }\n",
    "  pipeline.instantiate(HYPER_PARAMETERS)\n",
    "  pipeline = VoiceActivityDetection(segmentation=model)\n",
    "\n",
    "  vad = pipeline(audio_path)\n",
    "  diarization_df = pd.DataFrame()\n",
    "  speakers = []\n",
    "  sr = 16000\n",
    "  for turn, _, speaker in vad.itertracks(yield_label=True):\n",
    "          if speaker not in speakers:\n",
    "              speakers.append(speaker)\n",
    "\n",
    "          dur = turn.end-turn.start\n",
    "          new_row = {'start': int(turn.start * sr), \n",
    "                  'end': int(turn.end * sr), \n",
    "                  'start [s]': round(turn.start,3), \n",
    "                  'end [s]': round(turn.end,3), \n",
    "                  'dur [s]' : round(dur,3), \n",
    "                  'speaker': \"participant\"}\n",
    "          diarization_df = pd.concat([diarization_df, pd.DataFrame(new_row, index=[0])], \n",
    "                                  ignore_index=True)\n",
    "  # save to file\n",
    "  diarization_df.to_csv(audio_path[:-4] + \"_vad.csv\", sep=\" \", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'pipeline' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mvad_pyannote\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/aleph/diariziation_error_rate/filtered_missed_detection/Slachevsky_EP_26_Letra_P_manual_participant.wav\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[18], line 19\u001b[0m, in \u001b[0;36mvad_pyannote\u001b[0;34m(audio_path)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvad_pyannote\u001b[39m(audio_path):\n\u001b[1;32m     13\u001b[0m   HYPER_PARAMETERS \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# remove speech regions shorter than that many seconds.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_duration_on\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.0\u001b[39m,\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# fill non-speech regions shorter than that many seconds.\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_duration_off\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     18\u001b[0m   }\n\u001b[0;32m---> 19\u001b[0m   \u001b[43mpipeline\u001b[49m\u001b[38;5;241m.\u001b[39minstantiate(HYPER_PARAMETERS)\n\u001b[1;32m     20\u001b[0m   pipeline \u001b[38;5;241m=\u001b[39m VoiceActivityDetection(segmentation\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m     22\u001b[0m   vad \u001b[38;5;241m=\u001b[39m pipeline(audio_path)\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'pipeline' referenced before assignment"
     ]
    }
   ],
   "source": [
    "vad_pyannote(\"/home/aleph/diariziation_error_rate/filtered_missed_detection/Slachevsky_EP_26_Letra_P_manual_participant.wav\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
